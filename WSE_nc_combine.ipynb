{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import date, datetime, timedelta\n",
    "import json\n",
    "from shapely.geometry import box, mapping\n",
    "from geocube.api.core import make_geocube\n",
    "from geocube.rasterize import rasterize_points_griddata\n",
    "from functools import partial\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas.io.sql as psql\n",
    "import os\n",
    "import xarray as xr\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is loading environment variables from a .env and creating a connection to a database using the POSTGRES_DATABASE, POSTGRES_USER and POSTGRES_PASSWORD environment variables.\n",
    "\n",
    "It then sets some default time variables for reference. This is followed by setting time query range between modifiable min_date and max_date.\n",
    "\n",
    "It then fetches required data from Postgresql engine as pandas dataframe. The code then sets dtypes of certain fields like 'mile', 'lat_g' and 'lon_g' to simplify operations. At the same time, it also converts the time field to datetime in order to build a list of dates to iterate the WSE interpolation from. It uses function format_time() to convert the POSIX time to datetime string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading environment variables\n",
    "'''\n",
    "dotenv_path = Path('.env') # Load environment file\n",
    "load_dotenv(dotenv_path=dotenv_path) # Loads environment variables from .env file inside the project\n",
    "'''\n",
    "load_dotenv() # Loads environment variables from a .env file in the current working directory\n",
    "\n",
    "## Defining variables to connect to it\n",
    "dbname = os.environ.get(\"POSTGRES_DATABASE\") # Defines database name variable by retrieving value from environment variable\n",
    "user_db = os.environ.get(\"POSTGRES_USER\") # Defines user variable by retrieving value from environment variable\n",
    "password = os.environ.get(\"POSTGRES_PASSWORD\") # Defines password variable by retrieving value from environment variable\n",
    "host=\"10.3.10.19\" # Database host or IP address as string\n",
    "port=\"5432\" # Port number of database as string\n",
    "\n",
    "## Create connection\n",
    "engine = create_engine(f\"postgresql://{user_db}:{password}@{host}:{port}/{dbname}\") # Creates an engine connection with PostgreSQL using the variables above\n",
    "\n",
    "## Set some default time variables for reference\n",
    "today = date.today() # Gets today's date as date object\n",
    "yesterday = today - timedelta(days = 1) # Gets yesterday's date as date object\n",
    "now = datetime.now() # Gets current date and time as datetime object\n",
    "\n",
    "##Set time query\n",
    "min_date = '2021-05-24 00:00:00' # Modifiable - Sets minimum date for query\n",
    "min_date = pd.Timestamp(min_date).strftime('%Y-%m-%d') # Converts min_date from datetime object to a string\n",
    "max_date = '2023-01-01 00:00:00' # Modifiable - Sets maximum date for query\n",
    "max_date = pd.Timestamp(max_date).strftime('%Y-%m-%d') # Converts max_date from datetime object to a string\n",
    "\n",
    "## Retrieves data from rg_wse_3h_subset between min_date and max_date \n",
    "sql = f\"SELECT * FROM rg_wse_3h_subset WHERE time<'{max_date}'and time>'{min_date}'\" # and sid='rg_01160'\" # Optional condition to filter data further\n",
    "wse_df = psql.read_sql(sql, engine) # Executes SQL query and stores result as dataframe\n",
    "wse_df_types = wse_df.dtypes # Gets data types of columns in dataframe\n",
    "\n",
    "##Use the time field to build a list to iterate the wse interpolation from\n",
    "wse_df = wse_df.sort_values('time') # Sorts values in dataframe by time column\n",
    "wse_period_range = wse_df['time'].drop_duplicates().values.tolist() # Generates list of unique time values from dataframe\n",
    "\n",
    "##Convert POSIX time to datetime then string\n",
    "def format_time(time): # Defines function to convert POSIX time to datetime/string format\n",
    "    format = pd.to_datetime(time) # Converts POSIX time to datetime type\n",
    "    return format.strftime('%Y-%m-%d %H:%M:%S') # Converts datetime type to string\n",
    "\n",
    "wse_period_list = list((map(lambda t: format_time(t), wse_period_range))) # Generates list of strings from datetime objects\n",
    "resolution = r = 500 # Assign default resolution for following interpolation\n",
    "directory = 'Z:\\\\wse\\\\' # Set directory location for output WSE surfaces\n",
    "directory_str = directory + f'wse_{r}m_' # Set directory path string for combined daily netcdf output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp(min_date) + timedelta(days = 1)\n",
    "end = pd.Timestamp(max_date) - timedelta(days = 1)\n",
    "# Find date range of days in the given range \n",
    "day_list = []\n",
    "day_range = pd.date_range(start=start, end=end, freq=\"D\")\n",
    "\n",
    "for d in day_range:\n",
    "    day_list.append('y' + d.strftime('%Y') + '_d' + d.strftime('%j'))\n",
    "\n",
    "# Create period_dict to store the output data \n",
    "period_dict = {}\n",
    "\n",
    "# Loop over the day_range\n",
    "for d in day_range:\n",
    " \n",
    "    # Set previous step to 3 hour earlier from the d\n",
    "    previous_step = d - timedelta(hours=3)\n",
    "    \n",
    "    # Set next step to 1 day from the d\n",
    "    next_step = d + timedelta(days=1)\n",
    "    \n",
    "    # Initialize dictionary with string version of 'd' as key\n",
    "    period_dict[d.strftime('y%Y_d%j')] = []\n",
    "    \n",
    "    # Find date range of the hours in between previous_step and next_step with the frequency of 3 hours\n",
    "    range = pd.date_range(start=previous_step, end=next_step, freq='3H')\n",
    "\n",
    "    # Loop over the range \n",
    "    for t in range:\n",
    "        # Make filename template by filling in the required data\n",
    "        stamp = t.strftime('%Y%m%dT%H%M')\n",
    "        day = t.strftime('%j')\n",
    "        #filename_template = \"{directory_str}{stamp}_y{t.year}_d{t.day_of_year}.nc\"\n",
    "        filename_template = \"{directory_str}{stamp}_y{t.year}_d{day}.nc\"\n",
    "        \n",
    "        # Execute format on the filename_template \n",
    "        ind_period = filename_template.format(directory_str=directory_str, stamp=stamp, t=t, day=day)\n",
    "        \n",
    "        # Store the ind_period in the period_dict with the selected key\n",
    "        period_dict[d.strftime('y%Y_d%j')].append(ind_period)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the wse_interp function which is used to interpolate missing Water Surface Elevations (WSE) for a given period of time. With time series gauge data stored in a pandas dataframe ('wse_df') as input, the script outputs WSEs converted to a geo-referenced format in netCDF. The process involves interpolating missing values between known elevation readings for a given time period, rasterizing the result using gridding formulas, and exporting the results with naming conventions that identify the selection and timeframe used. \n",
    "\n",
    "It begins by passing the period selection to create a subset dataframe, and then it removes a specific gage at Cape Giradeau to fix conflicts with the related river mile join.\n",
    "\n",
    "It then imports a river mile GeoJSON into a GeoDataFrame (GDF) and drops matching columns to simplify the following join. The river mile GDF is rounded to 1 decimal place (to account for any floating precision errors), sorted, and reindexed.\n",
    "\n",
    "The WSE dataframe is then merged with the river mile GDF using the 'mile' field as a key. After that, a column is added for the period and calculated as the maximum value of the time field, converted to an integer, which establishes a time dimension for each point that remains persistent through downstream interpolation.\n",
    "\n",
    "The GDF is then subset to limit its spatial domain to south of RM 1000 in the vicinity of Cape Giradeau, Missouri and the missing z-values are interpolated using a 1-D linear relationship between river mile and known WSE values.\n",
    "\n",
    "User-modified variables, such as a bounding box and projection with resolution, are set, and the GDF is formatted as a GeoCube. A POSIX time variable is assigned and expanded as a valid time field. The GeoCube is then clipped to the extent of the Mississippi River and exported as a .NetCDF file.\n",
    "\n",
    "For the function to work successfully, the script requires the Pyresampled, Geopandas, Pandas, Xarray, MatPlotLib, Cartopy libraries to be properly installed.\n",
    "\n",
    "*Note that some lines within this script are currently commented out and may require adjusting if using certain versions of software packages.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wse_interp(time_query):\n",
    "    '''This function uses passed period string, called time_query, to create a subset dataframe of Water Surface Elevation (WSE) values from a larger, pre-existing WSE dataframe.Given the subset WSE dataframe, the function then consolidates WSE values with River Mile measurements in a GeoDataFrame via a left/outer merge using river mile as the primary join field. Missing WSE values are interpolated based on a linear relationship between river mile and known WSE values. \n",
    "\n",
    "    The GeoDataFrame is then converted into an Xarray, which is further manipulated by dropping NaN values along river mile axis. Dimensions are expanded to incorporate time and user-entered variables such as bounding box, projection information and resolution. Output is exported as a netCDF file and saved to a specified path.'''\n",
    "        \n",
    "    # Pass period selection to create subset dataframe \n",
    "    period_select = str(time_query)\n",
    "    wse_slice_df = wse_df.loc[wse_df['time'] == period_select]\n",
    "    \n",
    "    # Remove gage at Cape Giradeau to fix conflict with river mile join\n",
    "    bad_gage = 'rg_CE401278'\n",
    "    wse_slice_df = wse_slice_df.loc[wse_slice_df['sid'] != bad_gage]\n",
    "    \n",
    "    # Import river mile .geojson into gdf and drop matching columns to simplify following join\n",
    "    url = 'https://raw.githubusercontent.com/hbienn/smartport_wse/main/'\n",
    "    rm_formatted = f'{url}/mr_rm_banks.geojson'\n",
    "    rm_gdf = gpd.read_file(rm_formatted, crs='epsg:4326')\n",
    "    rm_gdf = rm_gdf.drop(columns=['OBJECTID', 'ord', 'sid', 'wse', 'time', 'lat_g', 'lon_g', 'bank']) # add 'bank'\n",
    "    \n",
    "    # Round river mile to 1 decimal place to account for any floating precision errors\n",
    "    rm_gdf = rm_gdf.round({'mile':1})\n",
    "    rm_gdf = rm_gdf.sort_values('mile')\n",
    "    rm_gdf = rm_gdf.reindex()\n",
    "    \n",
    "    # Merge WSE df with river mile gdf using mile as key\n",
    "    wse_gdf = rm_gdf.merge(wse_slice_df, how='outer', on='mile')\n",
    "    \n",
    "    # Reorder columns for obsessive compulsive reasons\n",
    "    cols = wse_gdf.columns.tolist()\n",
    "    cols = ['mile', 'sid', 'z', 'time','lon', 'lat', 'lat_g', 'lon_g', 'geometry']\n",
    "    wse_gdf = wse_gdf[cols]\n",
    "    wse_gdf = wse_gdf.sort_values('mile')\n",
    "    \n",
    "    # Add column for period and calculate it as max of ['time'] and convert to integer. \n",
    "    # Establishes a time dimension for each point that is persistent through the downstream interpolation.\n",
    "    period = pd.to_datetime(time_query)\n",
    "    year = period.strftime(\"%Y\")\n",
    "    day_of_year = period.strftime(\"%j\")\n",
    "    period = int(round(period.timestamp())*1000000000)\n",
    "    wse_gdf.insert(4,'period', period)\n",
    "    \n",
    "    # Subset gdf to limit spatial domain to south of RM 1000 in the vicinity of Cape Giradeau, MO\n",
    "    wse_gdf = wse_gdf.loc[wse_gdf['mile'] <= 1000]\n",
    "    \n",
    "    # Interpolates missing WSE values based on a linear relationship between river mile and known WSE values.\n",
    "    #wse_gdf = wse_gdf.dissolve(by='mile', aggfunc='mean')\n",
    "    wse_gdf = wse_gdf.sort_values('mile')\n",
    "    wse_gdf['z'] = wse_gdf['z'].interpolate(method='linear', limit_direction = 'both')\n",
    "    \n",
    "    # User modified variables\n",
    "    bounding_box = json.dumps(mapping(box(-91.7,28.9,-89,38.8))) \n",
    "    projection = 'EPSG:26915'\n",
    "    \n",
    "    # Still issues here with getting make_geocube to recognize time field and assign it correct dtype (datetime64[ns]). \n",
    "    # Potentially results from use of a timezone-aware dtype, workaround implemented.\n",
    "    wse_xr = make_geocube(\n",
    "                        vector_data = wse_gdf,\n",
    "                        measurements = ['z',],\n",
    "                        #datetime_measurements=['period'],\n",
    "                        output_crs = projection,\n",
    "                        resolution = (r, r),\n",
    "                        geom = bounding_box,\n",
    "                        #interpolate_na_method='linear',\n",
    "                        rasterize_function=partial(rasterize_points_griddata, method='linear', filter_nan = True)\n",
    "                        )\n",
    "    # Expand dimensions and populate with the POSIX time value variable previously assigned \n",
    "    period = int(wse_gdf['period'].mean())\n",
    "    wse_xr = wse_xr.expand_dims('time')\n",
    "    arr = wse_xr['time'].to_numpy()\n",
    "    arr[0,] = period\n",
    "    wse_xr['time'] = arr\n",
    "    wse_xr['time'] = pd.to_datetime(wse_xr['time'],utc=True)\n",
    "    period_label = pd.to_datetime(time_query).strftime('%Y%m%dT%H%M')\n",
    "\n",
    "    # Clip surface to extent of Mississippi River\n",
    "    url = 'https://raw.githubusercontent.com/hbienn/smartport_wse/main/'\n",
    "    mr_formatted = f'{url}/generalized_nhdarea_stlouistogulf_utm.geojson'\n",
    "    mr = gpd.read_file(mr_formatted, crs=projection)\n",
    "    wse_xr = wse_xr.rio.clip(mr.geometry, mr.crs, drop=True, invert=False)\n",
    "    \n",
    "    # Export as .netcdf \n",
    "    object_name = f'wse_{resolution}m_{period_label}_y{year}_d{day_of_year}.nc'\n",
    "    file_for_upload = directory + object_name\n",
    "    wse_xr.to_netcdf(file_for_upload)\n",
    "        \n",
    "    return\n",
    "\n",
    "for t in wse_period_list:\n",
    "    if t != wse_period_list[-1]:\n",
    "        wse_interp(t)\n",
    "    if t == wse_period_list[-1]:\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is designed to combine multiple netCDF files into a single daily file for easier analysis. The current implementation iterates through a list (day_list) and combines the 10 netcdfs stored by the matching key value in a dictionary (period_dict). Each output netcdf includes interpolated WSE values for a given day (8) plus the previous (1) and next timestep (1). \n",
    "\n",
    "* day_list is a list of integers and associated str entries that represents the days of interest.\n",
    "* xr.open_mfdataset() is used to open multiple netCDF files at once. In this code, period_dict[str(i)] returns a list of file paths for the ith day in the day_list.\n",
    "* combine = 'by_coords' specifies that the files should be combined along the coordinate dimensions.\n",
    "pd.to_datetime() converts the time value at index 2 of the time variable in each netCDF file to a datetime object.\n",
    "* strftime('%Y%m%d') formats the date as YYYYMMDD string format.\n",
    "* output_name = f'wse_{r}m_{t}_combine.nc' creates the name for the output file, which includes the resolution r, date t, and file extension .nc.\n",
    "* ds.to_netcdf(directory + output_name) saves the combined netCDF file to the specified directory with the output_name as the file name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in day_list:\n",
    "    ds = xr.open_mfdataset(period_dict[str(i)], combine = 'by_coords')\n",
    "    t = pd.to_datetime(ds['time'].values[2]).strftime('%Y%m%d')\n",
    "    output_name = f'wse_{r}m_{t}_combine.nc'\n",
    "    ds.to_netcdf(directory + output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "def wse_QC(nc_path):\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "    plot = ds.z.where(ds.z!=ds.z.rio.nodata).plot()\n",
    "    plot.axes.set_xlabel('Easting (m)')\n",
    "    plot.axes.set_ylabel('Northing (m)')\n",
    "    title = pd.to_datetime(int(ds['time'])).strftime('%Y%m%dT%H%M')\n",
    "    plot.axes.set_title(str(title))\n",
    "    plot.axes.grid(True, linestyle='dotted')\n",
    "    plot.axes.tick_params()\n",
    "    plot.axes.ticklabel_format(axis = 'both', style='sci', scilimits=(0,0))\n",
    "    plot.colorbar.set_label('WSE (NAVD88 m)')\n",
    "    plot.axes.annotate('PCS: NAD83/UTM Zone 15N', xy=(5,5), xycoords='figure pixels',fontsize=7 )\n",
    "    plot.figure.set_dpi(150) # 300 is best for export\n",
    "\n",
    "    return plot\n",
    "\n",
    "wse_QC(r'Z:\\wse\\testing\\wse_1000m_20220902_combine.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46cbcb8dccb081b74e3c679f2e2bc233d33682670de62d81c0f1a14e2c6e6726"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
